# Superconnect configuration
# Docs: https://github.com/bitovi/superconnect#readme

[inputs]
figma_file_url = "https://example.com/figma/angular-sample"
component_repo_path = "."
# Also requires FIGMA_ACCESS_TOKEN env var

[agent]
# Backend for code generation:
#   "anthropic-agent-sdk"     (default) — Claude explores your codebase using tools
#   "anthropic-messages-api"  — Anthropic Messages API (curated context)
#   "openai-chat-api"        — OpenAI Chat Completions API or compatible provider
api = "anthropic-agent-sdk"
model = "claude-sonnet-4-5"

# Alternative backends:
#   api = "anthropic-messages-api"   # Messages API (deterministic context)
#   api = "openai-chat-api"
#   model = "gpt-5.2-codex"
#   llm_proxy_url = "http://localhost:4000/v1"  # LiteLLM, Azure, vLLM, LocalAI

[codegen]
# How many times to retry if generated code fails validation (0-10)
max_retries = 4

# Parallel LLM requests during code generation (1-16, higher = faster but more API load)
concurrency = 8

# Place Code Connect files next to source components (default: true)
# When true: button.component.ts → button.component.figma.ts in same directory
# When false: all files go to code_connect_output_dir
# colocation = true

# Where to write Code Connect files when colocation = false (default: codeConnect/)
# code_connect_output_dir = "codeConnect"

[figma]
# How deep to scan Figma's component tree. Increase if nested variants aren't detected.
# layer_depth = 3
